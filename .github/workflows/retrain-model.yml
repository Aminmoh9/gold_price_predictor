# Automated weekly model retraining with deployment to S3

name: Weekly Model Retrain

on:
  schedule:
    - cron: '0 2 * * 0'  # Every Sunday at 2:00 AM UTC
  workflow_dispatch:  # Allow manual trigger

env:
  PYTHON_VERSION: '3.12'
  TF_CPP_MIN_LOG_LEVEL: '2'

jobs:
  retrain:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Run training script
        id: training
        env:
          SNS_TOPIC_ARN: ${{ secrets.SNS_TOPIC_ARN }}
        run: |
          python src/gold_price_train.py
          echo "✅ Training completed successfully"

      - name: Upload model to S3
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          
          # Upload with timestamp (versioning)
          aws s3 cp models/gold_lstm_model.keras s3://${{ secrets.S3_BUCKET }}/models/gold_lstm_model_${TIMESTAMP}.keras
          aws s3 cp models/gold_scaler.pkl s3://${{ secrets.S3_BUCKET }}/models/gold_scaler_${TIMESTAMP}.pkl
          aws s3 cp models/model_metadata.json s3://${{ secrets.S3_BUCKET }}/models/model_metadata_${TIMESTAMP}.json
          
          # Upload as "latest"
          aws s3 cp models/gold_lstm_model.keras s3://${{ secrets.S3_BUCKET }}/models/latest/gold_lstm_model.keras
          aws s3 cp models/gold_scaler.pkl s3://${{ secrets.S3_BUCKET }}/models/latest/gold_scaler.pkl
          aws s3 cp models/model_metadata.json s3://${{ secrets.S3_BUCKET }}/models/latest/model_metadata.json
          
          echo "✅ Models uploaded to S3"

      - name: Upload MLflow logs to S3
        run: |
          aws s3 sync mlruns/ s3://${{ secrets.S3_BUCKET }}/mlruns/ --delete
          echo "✅ MLflow logs uploaded to S3"

      - name: Read training metrics
        id: metrics
        run: |
          MAPE=$(python -c "import json; print(json.load(open('models/model_metadata.json')).get('mape', 'N/A'))")
          echo "mape=$MAPE" >> $GITHUB_OUTPUT
          echo "✅ MAPE: $MAPE%"

      - name: Send SNS notification (if configured)
        if: success()
        continue-on-error: true
        env:
          SNS_ARN: ${{ secrets.SNS_RETRAIN_ARN }}
        run: |
          if [ -n "$SNS_ARN" ]; then
            aws sns publish \
              --topic-arn "$SNS_ARN" \
              --subject "✅ Gold Model Retrained Successfully" \
              --message "Model retraining completed on $(date). MAPE: ${{ steps.metrics.outputs.mape }}%"
            echo "✅ SNS notification sent"
          else
            echo "⚠️ SNS_RETRAIN_ARN not configured - skipping notification"
          fi

      - name: Summary
        if: always()
        run: |
          echo "## Training Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ${{ job.status }}" >> $GITHUB_STEP_SUMMARY
          echo "- **MAPE:** ${{ steps.metrics.outputs.mape }}%" >> $GITHUB_STEP_SUMMARY
          echo "- **S3 Bucket:** ${{ secrets.S3_BUCKET }}" >> $GITHUB_STEP_SUMMARY
